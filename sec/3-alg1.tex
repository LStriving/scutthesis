\chapter{基于关键点序列的吞咽造影视频分析}

\begin{figure*}[tp]
    \vspace{-5pt}
    \centering
    \includegraphics[width=1.0\textwidth]{fig/alg1/framework_v6 (cropped).pdf}
    \caption{本章所提出的SG-Mamba框架示意图。}
    \label{fig:framework}
\end{figure*}

\section{问题定义与方法概述}

% \textbf{Problem definition.}
给定一个未裁剪的吞咽视频 $V$，我们使用滑动窗口方法将其划分为重叠的片段 $V = \{v_1, v_2, \dots, v_s\}$，其中每个片段 $v_i$ 包含固定数量的连续帧（例如，步长为 3 的 8 帧）。时序微动作定位的任务是预测不同类型吞咽微动作的开始和结束时间。设标签 $Z=\{z_1, z_2, \dots, z_{N}\}$ 表示视频中的一组微动作实例，其中 $N$ 代表实例的数量，因视频而异。每个实例 $z_i=(s_i, e_i, a_i)$ 由其开始时间 $s_i$、结束时间 $e_i$ 和动作标签 $a_i$ 定义，其中 $a_i \in \{1, \dots, C\}$（$C$ 为预定义的微动作类别数量），且 $s_i < e_i$。这项任务具有挑战性：在时间上，微动作持续时间极短且相互重叠；在空间上，图像中的细微运动、背景噪声和模糊轮廓阻碍了关键解剖结构的识别和精确的动作定位。

为了解决这些挑战，我们提出了一个名为 \fullname (\textbf{\sexyname}) 的多模态融合框架。遵循 Ruan \textit{et al.}~\cite{ruan2023temporal} 的方法，我们采用由粗到细的定位策略来简化微动作定位并获得更好的性能。具体而言，我们首先在粗略阶段使用现成的检测器（如 ActionFormer~\cite{zhang2022actionformer} 或 ActionMamba\cite{chen2024video}）获取整个吞咽动作的候选区域（proposals），然后在精细定位阶段，通过将候选区域的质心向外扩展固定的持续时间（例如 4 秒）来裁剪视频。粗略阶段的标签为 $Z^{1}=\{z^1_1,z^1_2,\ldots,z^1_M\}$，其中 $M = \frac{N}{C}$ 代表吞咽实例的总数。每个 $z^1_i=(s_i,e_i)$ 由整个吞咽事件的开始时间 $s_i$ 和结束时间 $e_i$ 定义。第二阶段的标签表示为 $Z^2 = \{z^2_1, \dots, z^2_C\}$。粗略阶段是一个简单的二分类时序动作定位任务，不是本文的重点，因此不再赘述。

对于细粒度定位，我们引入骨架热图作为编码解剖学先验的一种新颖模态。我们跟踪八个具有临床意义的关键点，以形成结构化的骨架序列，在增强解剖学关注度的同时抑制背景噪声。这取代了仅依赖外观数据的做法。
在获得外观特征和骨架特征后，我们设计了一个 \crossblockfull(\crossblockshort) 模块来有效地融合这些多模态特征，该模块以线性复杂度建立了跨模态的时间-上下文依赖关系。
% The features are then fed into respective detectors for localization. The predicted logits and offsets from both branches are fused to yield a more precise localization result. 
融合后的特征由并行检测器处理，其输出的 Logits 和偏移量被组合以产生精确的微动作定位预测。
我们的 \sexyname 的整体框架如图~\ref{fig:framework} 所示。

\section{预备知识}
由于我们的方法广泛使用了 Mamba 模块，首先介绍基于状态空间模型（State-Space Models, SSMs）的模型，特别是 S4 \cite{gu2021efficiently} 和 Mamba \cite{gu2023mamba}。SSMs 是一类数学框架，旨在通过隐藏状态 $h(t) \in \mathbb{R}^N$ 将输入序列 $x(t)$ 映射到输出序列 $y(t)$。形式上，SSMs 由常微分方程（ODEs）控制：
\begin{equation}
    h'(t) = \mathbf{A}h(t) + \mathbf{B}x(t), \quad y(t) = \mathbf{C}h(t),
\end{equation}
其中 $\mathbf{A} \in \mathbb{R}^{N \times N}$ 是状态转移矩阵，$\mathbf{B} \in \mathbb{R}^{N \times 1}$ 和 $\mathbf{C} \in \mathbb{R}^{1 \times N}$ 是投影矩阵。SSM 独立应用于 $x(t)\in \mathbb{R}^D$ 的每个通道。

Mamba~\cite{gu2023mamba} 是离散 SSM 的一种变体，它包含一个时间尺度参数 $\Delta$，通过零阶保持（Zero-Order Hold, ZOH）将连续参数 $\mathbf{A}$ 和 $\mathbf{B}$ 转换为离散对应项 $\overline{\mathbf{A}}$ 和 $\overline{\mathbf{B}}$：
\begin{equation}
    \overline{\mathbf{A}} = \exp(\Delta \mathbf{A}),\quad
    \overline{\mathbf{B}} = (\Delta \mathbf{A})^{-1}(\exp(\Delta \mathbf{A}) - \mathbf{I}) \cdot \Delta \mathbf{B}.
\end{equation}
这些变换使得隐藏状态 \(h_t\) 可以通过离散化的状态空间公式迭代计算：
\begin{equation}
    h_t = \overline{\mathbf{A}} h_{t-1} + \overline{\mathbf{B}} x_t, \quad y_t = \mathbf{C} h_t.
\end{equation}
上述操作可以通过全局卷积范式高效实现：
\begin{equation}\label{eq:ssm}
    \overline{\mathbf{K}}=(\mathbf{C}\overline{\mathbf{B}},\mathbf{C}\overline{\mathbf{A}\mathbf{B}},\ldots,\mathbf{C}\overline{\mathbf{A}}^{S-1}\overline{\mathbf{B}}),\quad\mathbf{Y}=\mathbf{X}*\overline{\mathbf{K}}
\end{equation}
其中 $S$ 表示序列的长度，$\mathbf{X} \in \mathbb{R}^{S \times D}$ 和 $\mathbf{Y} \in \mathbb{R}^{S \times D}$ 分别是输入和输出序列。与依赖静态参数的传统 SSM 不同，Mamba 引入了选择性扫描机制（Selective Scan Mechanism, S6）作为其核心算子，引入了动态上下文敏感性和自适应权重调制。在这里，$\mathbf{B}\in\mathbb{R}^{S \times D \times N}$，$\mathbf{C}\in\mathbb{R}^{S \times D \times N}$ 和 $\Delta\in\mathbb{R}^{S \times D}$ 是直接从输入 $\mathbf{X}\in \mathbb{R}^{S \times D}$ 推断出来的，从而能够有效处理不同的输入分布。为方便起见，我们使用 $\mathbf{Y}={\rm SSM}(\mathbf{X})$ 来表示公式~\ref{eq:ssm}。该公式也可在公式~\ref{eq:fw_ssm} 中复用。

\section{本章方法}
\subsection{解剖结构先验集成}

为了解决 VFSS 中外观特征的固有局限性，我们引入解剖结构先验作为一种新模态。该模态显式编码了临床上关键的结构（例如舌骨），这些结构已知是吞咽微动作的锚点，提供了空间引导以抑制 X 射线成像中的噪声并解决模糊性，如图~\ref{fig: heatmap illu} 所示。与仅依赖外观数据的现有方法不同，我们对特定领域解剖先验的整合使得模型能够有针对性地关注关键结构短暂但对诊断至关重要的运动。}

我们通过骨架关键点来整合这些解剖先验，这些关键点的选择基于其与吞咽动力学的临床相关性。
具体而言，我们标注了八个对吞咽至关重要的关键点，涵盖了舌骨、软腭和颈椎区域（见图~\ref{fig: motivation}(b)）。
% Our method extends to other domains by replacing these with domain-specific landmarks (\textit{e.g.}, a sports player’s body joints). 
关键点由两名专家手动标注并核实以确一致性。为了确保实用性，我们仅在一部分训练视频上标注了关键点。
% Additional details on spatial keypoint annotations and pre-processing are provided in the supplementary.
然后，我们训练了一个基于 HRNet 的关键点检测器 \cite{wang2020deep}，用于在时序定位的训练和测试期间提取关键点。

为了提高关键点的时间稳定性，我们采用了卡尔曼滤波（Kalman filter）\cite{kalman1960new}，以减少噪声并平滑轨迹，从而提高吞咽过程中的跟踪精度。

从每个视频帧提取关键点后，我们将它们编码为热图以提高微动作定位的鲁棒性。每个 2D 关键点表示为一个三元组 $\{(x_k, y_k, c_k)\}$，其中第 $k$ 个关键点具有坐标 $(x_k, y_k)$ 和置信度分数（最大热图响应）$c_k$。这些关键点被转换为热图 $h \in \mathbb{R}^{W \times H \times K}$，其中 $W$ 和 $H$ 是帧的宽度和高度，$K = 8$ 是关键点的数量。具体来说，关键点热图 $h^J$ 推导如下，包含 $K$ 个以每个关键点为中心的高斯分布：
\begin{equation}\label{eq: heatmap_k}
    h^J_{ijk} = \exp\left(-\frac{(i - x_k)^2 + (j - y_k)^2}{2 \sigma^2}\right) \cdot c_k.
\end{equation}
% where the standard deviation is set to $\sigma = 4$.
此外，我们导出了线热图 $h^L \in \mathbb{R}^{W \times H \times L}$，其中 $L$ 是连接关键点的线的数量，公式如下：
\begin{equation}\label{eq: heatmap_l}
h^L_{ijl} = \exp\left(-\frac{{\rm dist}((i, j), {\rm seg}(\alpha, \beta))}{2 \sigma^2}\right) \cdot \min(c_{\alpha}, c_{\beta}),
\end{equation}
其中第 $l$ 条线表示关键点 $\alpha$ 和 $\beta$ 之间的线段 ${\rm seg}(\alpha, \beta)$，${\rm dist}$ 是从位置 $(i, j)$ 到线段 $\text{seg}(\alpha, \beta)$ 的距离。为了减少热图冗余，我们遵循 Hyder \textit{et al.} \cite{hyder2024action} 的方法，对关键点热图 $h^J: \mathbb{R}^{W \times H \times K} \to \mathbb{R}^{W \times H}$ 和线热图 $h^L:\mathbb{R}^{W \times H \times L} \to \mathbb{R}^{W \times H}$ 进行通道维度的求和。如表~\ref{tab: heatmap input ablation} 所示，结合关键点和线热图产生了最佳结果。因此，对于帧 $f_i$ 的 2D 关键点，最终组合的“骨架”热图 $h_i \in \mathbb{R}^{W \times H}$ 定义为：$h_i = (h_i^J + h_i^L)/{2}$。图~\ref{fig: heatmap illu} 展示了这三种类型热图的示例。
为了进一步压缩空间冗余，我们将骨架热图裁剪为包含所有视频序列关键点的最小边界框，并将其调整为目标分辨率（例如，$56^2$）。3D 热图体积是通过在时间上堆叠这些处理后的 2D 热图构建的。

片段 $v_i$ 的 3D 骨架热图 $h^{3D}_i$ 通过来自 VideoMamba~\cite{li2025videomamba} 的三个双向 Mamba 块层进行处理以提取特征。
首先，$h^{3D}_i$ 通过基于 3D 卷积的 Patch Embedding 进行标记化（tokenized），并辅以时空位置编码。
然后，这些 Token 依次通过三个双向 Mamba 层以建模时空依赖关系。
最后，通过对第三层后的所有 Token 进行平均池化来获得骨架特征 $f^{ske}_i \in \mathbb{R}^d$：
\begin{equation}\label{eq: heatmap feature}
f^{ske}_i = {\rm AvgPooling}(\mathcal{F}_{\text{BM}}(h^{3D}_i)),
\end{equation}
其中 $\mathcal{F}_{\text{BM}}(\cdot)$ 是三层双向 Mamba 块。

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/alg1/heatmap_example.png}
    \caption{不同热图输入的示例。我们在 RGB 帧上绘制了关键点（蓝色）以作说明。}
    \label{fig: heatmap illu}
\end{figure}

\subsection{基于 \crossblockshort 的多模态交互}
\noindent\textbf{外观表示。} 视频片段的外观特征是使用预训练的 I3D 模型 \cite{carreira2017quo} 提取的，遵循 \cite{ruan2023temporal} 的方法。外观特征可以表示为：
\begin{equation}\label{eq: app feature}
f^{app}_i = [\mathcal{F}_{\text{rgb}}(v_i)\|\mathcal{F}_{\text{flow}}(v_i^{flow})],
\end{equation}
其中 $\|$ 表示拼接。$\mathcal{F}_{\text{rgb}}$ 和 $\mathcal{F}_{\text{flow}}$ 分别表示用于提取 RGB 和光流特征的 I3D 模型，$v_i^{flow}$ 是光流。

\noindent\textbf{\crossblockfull。}
我们提出了 \crossblockfull(\crossblockshort) 模块以增强外观特征和骨架特征之间的交互。\crossblockshort 集成了选择性扫描机制（S6）来建模跨模态的时间-上下文依赖关系。给定来自 $T$ 个片段的视频 $V$ 的外观特征 $F^{app} = [f_i^{app}]_{i=1}^T$ 和骨架特征 $F^{ske} = [f_i^{ske}]_{i=1}^T$，我们的 \crossblockshort 通过双向 SSM 与另一种模态进行交互来增强每种模态。例如，为了增强 $F^{app}$，我们首先使用前向和后向 SSM，生成中间特征如下：
\begin{align}\label{eq:fw_ssm}
    F^{app}_{\text{fw}} &= \mathcal{F}_{\text{fw}}(F^{app},F^{ske})\\
    &= {\rm SSM}\left({\rm SiLU}({\rm Conv}(\psi(F^{app})))\right) \odot {\rm SiLU}(\psi(F^{ske})),\nonumber\\
    F^{app}_{\text{bw}}&={\rm flip}\left(\mathcal{F}_{\text{fw}}({\rm flip}(F^{app}),{\rm flip}(F^{ske}))\right),
\end{align}
其中 $F^{app}_{\text{fw}}$ 和 $F^{app}_{\text{bw}}$ 分别表示前向和后向 SSM 的过程。${\rm SiLU}$ 是 Sigmoid 线性单元激活函数 \cite{elfwing2018sigmoid}，${\rm Conv}$ 表示一维深度卷积操作，$\psi$ 是线性投影层，$\odot$ 表示逐元素乘法，${\rm flip}$ 对序列特征进行时间翻转。通常，$F^{app}_{\text{fw}}$ 和 $F^{app}_{\text{bw}}$ 通过直接求和 \cite{li2025videomamba} 或通道拼接 \cite{chen2024video} 进行聚合，然后通过线性投影生成 Mamba 的输出。然而，Mamba 模块表现出通道间缺乏交互，这可能导致跨通道的潜在信息冗余~\cite{tan2019efficientnet,wang2020orthogonal}。
% Inspired by SE \cite{hu2018squeeze} and ECA \cite{wang2020eca} modules, 
受 SE \cite{hu2018squeeze} 模块的启发，我们在拼接 $F^{app}_{\text{fw}}$ 和 $F^{app}_{\text{bw}}$ 之后引入了一个新颖的 \channelblockfull(\channelblockshort) 模块，构成了我们要完整的 \crossblockshort 模块：
\begin{equation}
\begin{aligned}\label{eq: ccm}
    {\rm CCM}(F^{app},F^{ske})=F^{app}+\psi_{\text{out}}(\mathcal{C}([F^{app}_{\text{fw}}\|F^{app}_{\text{bw}}])),\\
    \text{with} \quad \mathcal{C}(\mathbf{X})={\rm}\mathbf{X}+\gamma\odot\left(\mathbf{X}-{\rm SiLU}({\rm Conv_{1}(\mathbf{X}))}\right),
\end{aligned}
\end{equation}
其中 $ \psi_{\text{out}} $ 是一个降维线性投影层，将特征通道维度从 $2D$ 压缩到 $D$。$ \mathcal{C} $ 表示通道级增强过程，$ \gamma $ 是一个可学习的通道级缩放因子，初始化为一个较小的值（例如 $10^{-5} $）。$ {\rm Conv_1} $ 是核大小为 1 的 1D 卷积，产生单个输出通道。我们的 $\mathcal{C} $ 使得每个通道能够减去冗余的“共享”信息，从而增强每个通道的表达能力。\textbf{同样地}，我们可以通过 ${\rm CCM}(F^{ske}, F^{app})$ 增强骨架特征 $F^{ske}$。最后，我们应用若干层（例如 3 层）的 \crossblockshort 来获得最终的增强外观特征 $E^{app}$ 和骨架特征 $E^{ske}$，用于下游的微动作检测器。

\subsection{整体优化}

一旦我们获得最终的增强外观特征 $E^{app}$ 和骨架特征 $E^{ske}$，我们将它们输入下游的微动作检测器 \(\mathcal{G}\)，例如 ActionMamba \cite{chen2024video}。对于外观分支，检测器 \(\mathcal{G}^{app}\) 以增强的外观特征 \(E^{app}\) 作为输入，并为每个时间步 \(t\) 生成预测的开始时间 \(\hat{s}_t^{app}\)、结束时间 \(\hat{e}_t^{app}\) 和输出 Logit \(\theta_t^{app}\)，这可以写成：
\begin{equation}\label{eq: output}
\{(\hat{s}_i^{app}, \hat{e}_i^{app}, \theta_i^{app})\}_{i = 1}^T=\mathcal{G}^{app}(E^{app})
\end{equation}
同样，我们可以获得预测 \(\{\left(\hat{s}_i^{ske}, \hat{e}_i^{ske}, \theta_i^{ske}\right)\}_{i = 1}^T\)。值得注意的是，由于微动作的高度重叠，单个检测器无法精确捕捉复杂的细节 \cite{ruan2023temporal}。我们遵循 \cite{ruan2023temporal} 使用多个独立的微动作检测器以获得更好的性能。

遵循 \cite{zhang2022actionformer} 和 \cite{chen2024video}，我们采用 Focal Loss \cite{tian2019fcos} 进行二分类，采用 DIoU Loss \cite{rezatofighi2019generalized} 进行边界回归以增强时间精度。对于外观分支，损失 $\mathcal{L}_{app}$ 是分类损失 $\mathcal{L}_{cls}^{app}$ 和回归损失 $\mathcal{L}_{reg}^{app}$ 的和。最终的损失是两个分支损失的总和。

\noindent\textbf{模型推理的后期融合。} 在推理阶段，我们从外观和骨架分支获得预测结果。预测的开始时间 $\hat{s}_t$、结束时间 $\hat{e}_t$ 和类别概率 $\hat{p}_t$ 计算如下：
\begin{align}\label{eq: late fusion}
\notag\hat{s}_i &= \omega \hat{s}_i^{app} + \left(1-\omega\right) \hat{s}_i^{ske},\quad
\hat{e}_i = \omega \hat{e}_i^{app} + \left(1-\omega\right) \hat{e}_i^{ske}, \\
\hat{\theta}_i &= \omega \hat{\theta}_i^{app} + \left(1-\omega\right) \hat{\theta}_i^{ske},\quad
\hat{p}_i = {\rm Sigmoid}(\hat{\theta}_i),
\end{align}
其中 $\omega $ 是一个加权超参数，${\rm Sigmoid}$ 表示用于将 Logits 转换为类别概率的 Sigmoid 激活函数。在获得融合的预测后，我们进一步应用 Soft-NMS \cite{bodla2017soft} 算法以去除高度重叠的实例。需要注意的是，最终的预测值需要结合粗定位阶段结果获得的偏移量。通过包括损失设计、推理融合和后处理在内的整体优化过程，我们的 \sexyname 框架能够有效地学习并预测 VFSS 视频中的时序微动作，提供更准确和可靠的结果。

\section{实验}
\noindent\textbf{数据集：}
我们使用了来自文献 \cite{ruan2023temporal} 的造影视频数据集，该数据集收集自中山大学附属第三医院。数据集包含来自 71 名受试者（25 名男性，46 名女性）的 847 个视频（总时长：508 分钟）。数据集按受试者以 4:1:1 的比例划分为训练集、验证集和测试集。它包含了七种吞咽微动作和完整吞咽过程的时序标注。每个微动作的平均持续时间总结在表~\ref{tab:avg_duration} 中。\ib{更多数据集细节在我们的补充材料中提供}。

\begin{table}[tb]
    \small
    \renewcommand{\arraystretch}{1.0}
    \renewcommand{\tabcolsep}{15pt}
    \centering
    \caption{吞咽微动作的平均持续时间。}
    \begin{tabular}{lr}
        \toprule
        微动作类别 & 平均时长 (s) \\
        \midrule
        口腔运送 (Oral Transit) \cite{dantas2009effect} & 0.865 \\
        软腭抬升 (Soft Palate Elevation) & 1.334 \\
        舌骨运动 (Hyoid Motion) \cite{molfenter2012temporal} & 1.674 \\
        UES开放 (UES Opening) \cite{molfenter2012temporal} & 0.786 \\
        吞咽启动 (Swallow Initiation) & 0.334 \\
        咽期运送 (Pharyngeal Transit) \cite{dantas2009effect} & 1.332 \\
        喉前庭闭合 (Laryngeal Vestibule Closure) \cite{park2010initiation} & 0.769 \\
        \bottomrule
    \end{tabular}
    \label{tab:avg_duration}
\end{table}


\noindent\textbf{关键点标注：} 我们从 210 个裁剪后的吞咽视频中提取了 13,895 帧。这些数据被划分为三个子集用于关键点定位：8,302 张图像（来自 131 个视频）用于训练，1,518 张图像（来自 16 个视频）用于验证，以及 4,075 张图像（来自 63 个视频）用于测试。标注工作由专家使用开源的 \textit{CVAT} 工具\footnote{https://github.com/cvat-ai/cvat} 完成。为了确保可靠性，我们采用了多步验证流程，包括标注者之间的交叉验证和一致性检查。}

\noindent \textbf{评价指标：}
我们使用 \textbf{平均精度均值 (mean Average Precision, mAP)} 作为主要指标来评估 TMAL 的性能 \cite{ruan2023temporal, zhang2023locating}。对于检测预测，精度 (Precision) 和召回率 (Recall) 定义为：
\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN},
\end{equation}
其中 $ TP $、$ FP $ 和 $ FN $ 分别表示真阳性、假阳性和假阴性。精度-召回率 (PR) 曲线是通过改变置信度阈值形成的，平均精度 (AP) 对应于该曲线下的面积，数值近似为：
\begin{equation}
AP = \int_{0}^{1} p(r) \, dr,
\end{equation}
其中 $ p(r) $ 是精度-召回率函数。对于多阈值评估，我们遵循文献 \cite{ruan2023temporal}，计算时序交并比 (tIoU) 阈值 $\tau \in \{0.1, 0.2, \dots, 0.7\}$ 下的 mAP。阈值 $\tau$ 下的特定类别 AP 是对所有 $N_{\text{class}}$ 个类别取平均值：
\begin{equation}
AP_\tau = \frac{1}{N_{\text{class}}} \sum_{c=1}^{N_{\text{class}}} AP_\tau^{(c)}.
\end{equation}
为了评估提取的关键点的准确性，我们使用 \textbf{关键点正确概率 (Probability of Correct Keypoint, PCK)}\cite{yang2012articulated, moskvyak2021semi}。如果预测位置落在距离真值位置的指定距离阈值内，则认为该关键点定位正确。我们采用严格的 PCK@0.02 阈值，即距离阈值为图像对角线长度的 2\%。}

\begin{table*}[tb]
\small
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{4pt}
\begin{center}
\caption{与最先进方法的比较。最佳性能以 \textbf{粗体} 高亮显示。$\dagger$ 表示该方法由我们使用 ActionMamba 检测器复现。}
\begin{tabular}{lccrrrrrrrr}
\toprule
\multirow{2}{*}{方法}       & \multirow{2}{*}{检测器}&  \multirow{2}{*}{骨架输入} & \multicolumn{8}{c}{mAP@tIoU 阈值} \\
     & &  & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & Avg.\\
\midrule
% P-GCN \cite{zeng2019graph} & P-GCN &  & 82.2	& 80.2 & 73.7 &	63.7&	51.6	&40.6&	26.4&	59.8 \\
A2Net \cite{yang2020revisiting} & A2Net & & 53.7 & 51.5 & 45.7 & 36.0 & 22.4 & 10.6 & 3.5 &  31.9 \\
ActionFormer \cite{zhang2022actionformer} & ActionFormer &  & 76.8	& 74.4 & 69.7 &	59.3 &	48.8 & 38.5 & 24.6 & 56.0 \\
TriDet \cite{shi2023tridet} & TriDet & & 79.6 & 76.6 & 72.4 & 63.7 & 53.1 & 40.9 & 26.2 & 58.9 \\
AdaTAD \cite{liu2024end} & ActionFormer & & 81.0 & 77.4 & 70.0 & 62.3 & 54.4 & 42.1 & 24.8 & 58.9 \\
ActionMamba \cite{chen2024video} & ActionMamba & & 82.0	& 79.3 & 74.3 &	67.4 &	53.1	&37.1&	19.9&	59.0 \\
% AdaTAD \cite{liu2024end} & AdaTAD & 81.1 & 76.8 & 70.4 & 62.7 & 49.9 & 36.4 & 19.4 & 56.7 \\ %1e-4 small adapter
% \todo{add AdaTAD}
\midrule
Ruan \textit{et al.} \cite{ruan2023temporal} & A2Net  &  & 70.9 & 67.5 & 62.5 & 55.0 & 46.1 & 31.6 & 15.8 & 49.9\\
Ruan \textit{et al.}$^\dagger$ \cite{ruan2023temporal} & ActionMamba  &  & 77.9& 75.0& 69.5& 62.2&  54.8& 45.2& 28.9& 59.1\\
Hyder \textit{et al.}$^\dagger$ \cite{hyder2024action} & ActionMamba   & \checkmark & 76.8& 74.6& 69.4& 62.0& 56.2& 45.2& 30.8& 59.3\\
\textbf{SG-Mamba}(本章方法)  & ActionFormer& \checkmark&82.7 & \textbf{80.1} & \textbf{75.9} & 67.1 & 58.4 & 49.2 & 33.5 & 63.8\\
\textbf{SG-Mamba}(本章方法) & ActionMamba & \checkmark & \textbf{83.1} & 78.5& 74.6 & \textbf{67.9} & \textbf{59.0} & \textbf{50.0} & \textbf{37.2} & \textbf{64.3}\\
% \midrule
% \sexyname (Oracle) 	& ActionMamba  &	\checkmark 	 &	\textbf{91.2} &	\textbf{86.3} & \textbf{83.7} & \textbf{75.7} & \textbf{66.9} &	\textbf{56.7} & \textbf{42.2} & \textbf{71.8} \\
\bottomrule
\end{tabular}%}
\label{tab: main}
\end{center}
\end{table}



\begin{table}[tp]
\small
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{5pt}
\begin{center}
\caption{外部数据集上的定量结果。最佳结果以 \textbf{粗体} 显示。$\dagger$ 表示该方法由我们使用 ActionMamba 检测器复现。}
\begin{tabular}{lrrrrrrrr}
\toprule
\multirow{2}{*}{方法}       & \multicolumn{8}{c}{mAP@tIoU 阈值} \\
     & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & Avg.\\
\midrule
ActionFormer\cite{zhang2022actionformer} & 59.1& 53.9& 48.7& 39.7& 28.8& 16.0& 9.4& 36.5\\ 
TriDet \cite{shi2023tridet} &  64.5 & 61.1 & 53.4 & 44.6 & 28.7 & 14.9 & 5.1 & 38.9 \\
ActionMamba\cite{chen2024video} & 65.8& 61.0& 56.0& 43.0& 25.0& 14.0& 8.0& 39.0\\
AdaTAD \cite{liu2024end} &  57.6 & 56.4 & 55.8 & 50.1 & 38.7 & 20.9 & \textbf{13.0} & 41.8 \\
\midrule
Ruan \textit{et al.}$^\dagger$ \cite{ruan2023temporal} & 67.8& 62.5& 58.5& 49.5& 25.9& 16.8& 7.0& 41.1\\
Hyder \textit{et al.}$^\dagger$ \cite{hyder2024action} & 69.1 & 63.4 & 58.8 & 52.3 & 34.8 & 18.7 & 12.5 & 44.2 \\
\sexyname (本章方法) & \textbf{73.1} & \textbf{70.4} & \textbf{64.7} & \textbf{58.5} & \textbf{39.1} & \textbf{39.1} & 6.0 & \textbf{50.1}\\
\bottomrule
\end{tabular}
\label{tab: external}
\end{center}
%\vspace{-10pt}
\end{table}
\begin{table}[t]
\small
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{5pt}
\begin{center}
\caption{{范式和模态的影响。“外观分支”和“骨架分支”分别表示仅使用基于外观和基于骨架的特征进行微动作检测的预测结果。
“C2F”指“由粗到细 (Coarse-to-Fine)”。“双分支（后期融合）”，与表~\ref{tab: CCM ablation} 中的相同，是指使用外观和骨架特征作为输入并使用公式~\ref{eq: late fusion} 进行推理融合的模型。}}
\begin{tabular}{llrrrrr}
\toprule
\multirow{2}{*}{方法}  & \multirow{2}{*}{范式}  &\multicolumn{5}{c}{mAP@tIoU} \\
    &   &  0.1&0.3 & 0.5 & 0.7 & Avg.\\
\midrule
ActionMamba~\cite{chen2024video} & 单阶段 &  82.0&74.3 & 53.1 & 19.9& 59.0\\
\midrule
骨架分支 & C2F~\cite{ruan2023temporal}  &  75.5&66.4  & 50.3  & 13.0 &  52.7\\
外观分支~\cite{chen2024video} & C2F~\cite{ruan2023temporal} & 80.7&71.2 & 58.3    & 33.1& 61.7   \\
\midrule
单分支 (拼接) & C2F \cite{ruan2023temporal}& 78.6&70.9& 56.1& 35.4& 61.2\\
双分支 (后期融合) & C2F~\cite{ruan2023temporal}  &  80.9&72.1   &57.5 &34.2 & 62.1  \\
\sexyname(本章方法)    & C2F~\cite{ruan2023temporal}   &  \textbf{83.1}&\textbf{74.6}  & \textbf{59.0}  &\textbf{37.2} &\textbf{64.3}  \\
\bottomrule
\end{tabular}%}
\label{tab: modality ablation}
\end{center}
%\vspace{-10pt}
\end{table}

\noindent\textbf{实施细节：}
我们使用 HRNet~\cite{wang2020deep} (HRNet-W32) 进行骨架提取，并使用相同的训练策略。我们采用 VideoMamba \cite{li2025videomamba}（576 维特征）进行端到端的骨架特征提取，结合 3 层我们的通道增强型 Cross-Mamba 以增强外观特征和骨架特征之间的交互。对于下游任务，我们遵循 ActionMamba \cite{chen2024video} 的设置，但移除了主干网络的最后一层，因为它是针对长时间动作的，对微动作定位没有贡献。优化器设置、学习率、权重衰减和预热策略保持不变。遵循 Ruan \textit{et al.}\cite{ruan2023temporal}，我们采用由粗到细的定位机制和针对 VFSS 的单类别分类。在粗略阶段，为了公平比较，我们利用相同的预训练 I3D \cite{carreira2017quo} 来提取外观特征。在精细定位阶段，所有模型均以 batch size 为 16 训练 45 个 epoch。关于超参数，公式~\ref{eq: heatmap_k} 和公式~\ref{eq: heatmap_l} 中的标准差 $\sigma$ 设为 4，公式~\ref{eq: ccm} 中 $\gamma$ 的初始值设为 $10^{-5}$，公式~\ref{eq: late fusion} 中的融合权重 $\omega$ 设为 0.6。第~\ref{sec:hyper} 节研究了超参数的选择。
实现了模型 EMA \cite{huang2017snapshot} 和梯度裁剪以进一步稳定训练。
所有实验均在使用 7 张 NVIDIA GeForce RTX 3090 GPU（24GB 显存）的设备上进行。
我们使用 calflops \cite{calflops} 计算参数量和 FLOPs。


\begin{table}[tp]
\small
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{5pt}
\begin{center}
\caption{\crossblockshort 的消融实验。“Self-Mamba”指的是在公式~\ref{eq: ccm} 中每个分支输入相同的模态。}
\begin{tabular}{lrrrrrrr}
\toprule
\multirow{2}{*}{设置}    &\multicolumn{5}{c}{mAP@tIoU 阈值} & \multirow{2}{*}{\makecell{参数量\\(M)}}& \multirow{2}{*}{\makecell{FLOPs\\(G)}}\\
  &  0.1&0.3 & 0.5 & 0.7 & Avg. & & \\
\midrule
双分支 &  80.9&72.1   &57.5 &34.2 & 62.1   & 38.5& 105.6\\
\midrule
+ Self-Mamba         &  80.7&72.5 & 56.7   & 34.1 & 61.7  & 122.2& 116.9\\
+ Cross-Attention \cite{vaswani2017attention}      & 81.2&72.5 & 56.6  & 35.1 & 62.4   & 188.7& 163.3\\
+ \crossblockshort (本章方法)  & \textbf{83.1}&\textbf{74.6}& \textbf{59.0} & \textbf{37.2}&\textbf{64.3}   & 109.2& 116.9\\

\bottomrule
\end{tabular}
\label{tab: CCM ablation}
\end{center}
%\vspace{-10pt}
\end{table}
\begin{table}[t]
\small
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{5pt}
\newcolumntype{y}{>{\columncolor{yellow!30}}r}
\begin{center}
\caption{\crossblockshort 中 \channelblockshort 的消融实验。“CE-Add”表示公式~\ref{eq: ccm} 中的操作更改为加法。}
\begin{tabular}{lrrrrrrr}
\toprule
\multirow{2}{*}{设置}    &\multicolumn{5}{c}{mAP@tIoU 阈值} & \multirow{2}{*}{\makecell{参数量\\(M) }}& \multirow{2}{*}{\makecell{FLOPs\\(G)}} \\
  &  0.1&0.3 & 0.5 & 0.7 & Avg. & & \\
\midrule
w/o CE  & 82.3&73.3  & 58.1  & 35.1 & 63.3  & 109.1& 116.9\\
\midrule
CE-Add  & 81.7&73.3  &  58.6  & 36.0 &   63.5   & 109.2& 116.9\\
SE \cite{hu2018squeeze} &  82.5&74.0 & 57.8 &36.0 & 63.7   & 115.9& 116.9\\
ECA \cite{wang2020eca}  & 73.5 & 59.0  & 36.8 & 63.9  \\
\channelblockshort (本章方法)  & \textbf{83.1}&\textbf{74.6}& \textbf{59.0} & \textbf{37.2}&\textbf{64.3}   & 109.2& 116.9\\
\bottomrule
\end{tabular}%}
\label{tab: CE ablation}
\end{center}
%\vspace{-10pt}
\end{table}


\subsection{与最先进方法的比较}

在表 \ref{tab: main} 中，我们将 \sexyname 与三类方法进行了比较：(1) 不同架构的 SOTA 时序动作定位 (TAL) 方法（基于 CNN：A2Net \cite{yang2020revisiting}, TriDet \cite{shi2023tridet}；基于 Transformer：ActionFormer \cite{zhang2022actionformer}, AdaTAD \cite{liu2024end}；基于 Mamba：ActionMamba \cite{chen2024video}），(2) 针对 VFSS 的\footnote{Jeong \etal~\cite{jeong2023application} 的方法专为裁剪后的吞咽视频设计且未开源，因此无法比较。}微动作定位方法~\cite{ruan2023temporal}，以及 (3) 适应于微动作定位的骨架增强型多模态方法~\cite{hyder2024action}。对于 TAL 方法，ActionMamba 取得了最强的基线（59.0\% 平均 mAP），证明了 Mamba 架构相比其他方法在时序建模方面的优势。对于 VFSS 特定方法，我们在相同设置下使用 ActionMamba 重新实现了 \cite{ruan2023temporal}（标记为 $\dagger$）。虽然他们原来的方法改进了 A2Net，但其性能优于 ActionMamba 的幅度有限，揭示了其架构依赖性和有限的泛化能力。对于多模态方法，Hyder \textit{et al.} 将热图特征向量与 RGB 特征向量拼接，并通过 1×1 卷积层融合，取得了 59.3\% 的平均 mAP，但未能充分利用跨模态线索。我们的方法取得了 64.3\% 的平均 mAP，优于所有三类方法：比现有的 TAL 方法 (ActionMamba) 高出 \textbf{+5.3\%}，比原始的前 VFSS 特定方法 (Ruan \textit{et al.} with A2Net) 高出 \textbf{+14.4\%}，比骨架增强的基线 (Hyder \textit{et al.}) 高出 \textbf{+5.0\%}。值得注意的是，我们的框架并不局限于特定的检测器。如表~\ref{tab: main} 所示，\sexyname 取得了一致的改进，并且在平均表现上优于 ActionFormer 基线 \textbf{7.8\%}。\ib{我们在补充材料中提供了每个微动作类别的额外可视化和详细评估结果。}}


\begin{table}[tp]
\small
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{4pt}
\begin{center}
\caption{不同骨架特征编码器的消融实验。I3D 是预训练的并在训练期间冻结，而 MViT 和 VideoMamba 是轻量级版本并进行端到端训练。}
  % \vspace{-5pt}
\begin{tabular}{llrrrrr}
\toprule
\multirow{2}{*}{编码器}     & \multirow{2}{*}{训练方式}&\multicolumn{5}{c}{mAP@tIoU 阈值} \\
  &   &0.1&0.3 & 0.5 & 0.7 & Avg.\\
\midrule
I3D \cite{carreira2017quo}                 &   冻结  &62.2&53.9   &36.8  & \textbf{14.8}& 42.7  \\
MViT \cite{fan2021multiscale}                &  端到端 &72.3&64.8 & 43.2  & 11.9  &  49.1  \\
VideoMamba \cite{li2025videomamba}          &    端到端 &\textbf{75.5}&\textbf{66.4} &\textbf{50.3} & 13.0&\textbf{52.7}  \\
\bottomrule
\end{tabular}
\label{tab: heatmap encoder ablation}
\end{center}
%\vspace{-10pt}
\end{table}


\begin{table}[tp]
\small
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{8pt}
\begin{center}
\caption{不同热图输入的影响。示例输入如图~\ref{fig: heatmap illu} 所示。最佳结果以 \textbf{粗体} 显示。}
\begin{tabular}{ccrrrrr}
\toprule
\multicolumn{2}{c}{热图} &\multicolumn{5}{c}{mAP@tIoU 阈值} \\
   关键点 &线 &  0.1&0.3 & 0.5 & 0.7 & Avg.\\
\midrule
\Checkmark&&  67.0&59.8  &40.3   & 11.6 & 45.7 \\
&\Checkmark&  73.1&64.4  &46.0   & \textbf{14.7}& 50.8  \\
\Checkmark&\Checkmark&  \textbf{75.5}&\textbf{66.4}  &\textbf{50.3}  &13.0 & \textbf{52.7} \\
\bottomrule
\end{tabular}
\label{tab: heatmap input ablation}
\end{center}
%\vspace{-10pt}
\end{table}

\subsection{外部验证}
为了验证我们方法的可靠性和鲁棒性，我们在从不同地点（铭心康复中心）收集的额外数据集上进行了实验。数据由 8 名受试者构成，并使用 VFSS 采集与分析系统（朗意医疗，中国广州）以 30 帧/秒的速度数字化录制为视频。两名经验丰富的临床医生独立评估了所有视频。视频总时长为 95 分钟。遵循 \cite{ruan2023temporal} 中的预处理程序和标注标准，我们获得了总共 48 个标注片段。微动作的平均持续时间为 1.1 秒。在不进行重新训练或微调的情况下，我们直接在外部数据集上评估在 \cite{ruan2023temporal} 训练集上训练的模型。
如表~\ref{tab: external} 所示，我们的方法（使用 ActionMamba 作为检测器）在大多数 tIoU 阈值上都取得了更好的性能。平均 mAP 比之前的 VFSS 方法提高了 9.0\%，比骨架增强方法提高了 5.9\%，证明了增强的时序定位鲁棒性。这些结果证实了我们的方法对真实临床环境中异构采集条件和不同患者特征具有优越的泛化能力。为了支持临床应用，我们开发了一个用于 VFSS 视频分析的智能原型系统（\ib{详见补充材料}）。这一实际实现展示了该方法在现实场景中的可用性。

\subsection{消融实验}
\label{sec: ablation}
我们评估了 \sexyname 中关键组件的效果，由于篇幅限制，仅报告 mAP@0.1、mAP@0.3、mAP@0.5、mAP@0.7 和 mAP@Avg.（0.1 到 0.7）。

\noindent\textbf{多模态建模的消融。} 如表~\ref{tab: modality ablation} 所示，与 \cite{ruan2023temporal} 中的发现一致，采用单类别分类的由粗到细定位策略优于单阶段多类别分类方法（mAP@Avg. 59.0\% → 61.7\%）。关于模态的影响，仅使用骨架分支表现欠佳，因为像 UES 开放这样的微动作很难仅从关键点运动中推断出来。外观分支表现较好，但对视觉噪声（例如，舌骨缺失）敏感。对编码器特征的简单拼接阻碍了优化过程，且未能有效利用骨架数据。使用双分支架构并进行后期融合（即，平均预测结果）略微提高了性能。相比之下，我们的 \sexyname 结合了 CCM 模块，利用了先验知识并收集了有效的表示，将 mAP@Avg. 从 62.1\% 提升至 64.3\%。


\begin{table}[tp]
\small
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{5pt}
\begin{center}
\caption{骨架热图提取器的性能和参数比较，以及它们集成到骨架分支 (Ske.) 和 \sexyname 框架后的性能。(PCK@0.02, FLOPs单位为 G, 参数量单位为 M)}
% 修改列定义：增加一列，将原来的第3列拆分为两个 c
\begin{tabular}{lrcclrrrrr}
\toprule
% 表头拆分
\multirow{2}{*}{提取器} & \multirow{2}{*}{\makecell{PCK}} & \multirow{2}{*}{FLOPs} & \multirow{2}{*}{参数量} & \multirow{2}{*}{方法} & \multicolumn{5}{c}{mAP@tIoU 阈值} \\
 & & & & & 0.1 & 0.3 & 0.5 & 0.7 & Avg.\\
\midrule
% Simple-Baseline 数据拆分
\multirow{2}{*}{\makecell{Simple-\\Baseline}~\cite{zhao2019m2det}} & \multirow{2}{*}{80.8} & \multirow{2}{*}{6.2} & \multirow{2}{*}{15.4} & Ske. & 65.9 & 60.8 & 47.3 & 20.0 & 49.6 \\
 & & & & 本章方法 & 81.5 & 72.4 & 58.8 & 36.5 & 63.0 \\
\midrule
% HRNet 数据拆分
\multirow{2}{*}{HRNet~\cite{wang2020deep}} & \multirow{2}{*}{82.2} & \multirow{2}{*}{7.7} & \multirow{2}{*}{28.5} & Ske. & 75.5 & 66.4 & 50.3 & 13.0 & 52.7 \\ 
 & & & & 本章方法 & \textbf{83.1} & \textbf{74.6} & \textbf{59.0} & \textbf{37.2} & \textbf{64.3}\\
\bottomrule
\end{tabular}
\label{tab: heatmap extractor}
\end{center}
\end{table}

\begin{table}[tp]
\small
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{5pt}
\begin{center}
\caption{使用/不使用卡尔曼滤波进行骨架稳定的性能。最佳结果以 \textbf{粗体} 显示。}
\begin{tabular}{lcrrrrr}
\toprule
\multirow{2}{*}{方法}&\multirow{2}{*}{\makecell[c]{卡尔曼\\滤波} } & \multicolumn{5}{c}{mAP@tIoU 阈值} \\
&& 0.1 & 0.3 & 0.5 & 0.7 & Avg.\\
\midrule
\multirow{2}{*}{骨架分支} & \XSolidBrush& 69.0 & 57.4 & 37.5 & 11.0 & 44.6 \\
 & \Checkmark & 75.5& 66.4 & 50.3 & 13.0 & 52.7 \\
 \midrule
\multirow{2}{*}{\makecell[c]{双分支\\ (后期融合)}} & \XSolidBrush& 80.4& 70.7 & 55.9 & 34.0 & 61.2 \\
  & \Checkmark & 80.9& 72.1 & 57.5 & 34.2 & 62.1 \\
\midrule
\multirow{2}{*}{\makecell[c]{\sexyname (本章方法)}} & \XSolidBrush& 82.1& 72.7 & 57.3 & 35.9 & 62.9 \\
  & \Checkmark & \textbf{83.1}& \textbf{74.6} & \textbf{59.0} & \textbf{37.2} & \textbf{64.3}\\
\bottomrule
\end{tabular}
\label{tab: kalman}
\end{center}
%\vspace{-10pt}
\end{table}


\noindent\textbf{不同融合模块的比较。} 在表~\ref{tab: CCM ablation} 中，“Self-Mamba”在公式~\ref{eq: ccm} 中使用相同的模态（而不是互补模态）作为输入，导致性能略有下降。虽然交叉注意力（Cross-Attention）相较于基线略微改善了结果（mAP@Avg. +0.3\%），但我们的 \crossblockshort 带来了显著的性能提升（mAP@Avg. \textbf{+2.2\%}）。
至关重要的是，\crossblockshort 仅用 109.2M 参数和 116.9G FLOPs 就实现了这一点，所需的参数量比交叉注意力（188.7M 参数，163.3G FLOPs）少 42\%，FLOPs 少 28\%。这些结果清楚地证实了性能增益不仅源于有效的跨模态交互，还源于 \crossblockshort 的计算效率，使其适用于资源受限的临床应用。

\noindent\textbf{\crossblockshort 中 \channelblockshort 模块的消融。} 移除通道增强（w/o CE）会导致显著的性能下降（mAP@Avg. -1.0\%），证实了其必要性。虽然 SE\cite{hu2018squeeze} 略微改善了结果，但我们的 \channelblockshort 在增强变体中以最低的参数量（109.2M）和有限的额外成本实现了最佳精度（64.3\% mAP@Avg.）。关键是，\channelblockshort 中的减法操作比加法变体（CE-Add）的平均 mAP 高出 +0.8\%，凸显了冗余抑制相对于特征强化的有效性。

\noindent\textbf{骨架分支不同骨架编码器的消融。}
我们评估了三种用于骨架特征提取的编码器架构：I3D \cite{carreira2017quo}，一种广泛用于视频特征提取的基于 CNN 的模型；MViT \cite{fan2021multiscale}，一种多尺度 Vision Transformer；VideoMamba \cite{li2025videomamba}，一种最先进的基于 Mamba 的方法。鉴于热图包含稀疏的视觉信息，我们对 MViT 和 VideoMamba 仅使用 3 层的浅层架构。如表~\ref{tab: heatmap encoder ablation} 所示，VideoMamba 表现出卓越的性能，实现了 52.7\% 的平均 mAP，而 I3D 为 42.7\%，MViT 为 49.1\%。这一性能优势证明了我们选择 VideoMamba 作为主要骨架特征编码器的合理性。

\noindent\textbf{骨架分支不同输入的消融。} 我们评估了使用不同热图输入的骨架分支的性能：仅关键点、仅线和骨架热图。使用 VideoMamba 进行特征提取。如表~\ref{tab: heatmap input ablation} 所示，骨架热图取得了最佳性能，其次是仅线热图，而关键点热图结果最差。结果证明了结合关键点和线热图的益处。

\begin{figure*}[t]
    \centering
    % Standard Deviation Effect
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/alg1/ablation/std_dev_effect.pdf}
        \caption{标准差 $\sigma$ 的影响}
        \label{fig:std_dev}
    \end{subfigure}
    \hfill
    % CCM Layers Effect
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/alg1/ablation/ccm_layers_effect.pdf}
        \caption{CCM 层数的影响}
        \label{fig:ccm_layers}
    \end{subfigure}
    \hfill
    % Fusion Weight Effect
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/alg1/ablation/fusion_weight_effect.pdf}
        \caption{融合权重 $\omega$ 的影响}
        \label{fig:fusion_weight}
    \end{subfigure}
    \caption{对所提出方法不同组件的超参数分析，以平均 mAP 衡量。(a) 骨架分支中标准差 $\sigma$ 的影响；(b) \crossblockshort 层数的影响；(c) 融合权重 $\omega$ 的影响。}
    \label{fig:ablation_studies}
\end{figure*}

\noindent \textbf{不同热图提取模型的消融。}
我们评估了不同骨架提取器的影响。HRNet~\cite{wang2020deep} 实现了比 SimpleBaseline~\cite{zhao2019m2det}（以 ResNet-18 \cite{he2016deep} 为主干）更高的 PCK。对于独立的骨架分支，使用 HRNet 在所有阈值上均优于 SimpleBaseline，展示了其捕捉关键解剖特征的优越能力。当集成到我们完整的 \sexyname 框架中时，使用 HRNet 进一步取得了最先进的结果，在计算量有限的情况下，mAP@Avg. 超过 SimpleBaseline 1.3\%。这验证了 HRNet 有效保留了对微动作定位至关重要的空间细节。因此，我们采用 HRNet 作为默认的骨架提取器。}

\begin{figure}[t] % 使用 figure* 环境
    \centering
    % 并排显示两个子图（无需 subcaption）
    \begin{minipage}[t]{0.49\textwidth} % 单栏宽度占比
        \centering
        \includegraphics[width=\textwidth]{fig/alg1/results-vis/xlarge_fontsize/result-visual-8_96.0_2021082501_liu2meng2_jian4kang1cha2ti3_2021_08_25_152802_32.png}
        % \captionof{subfigure}{(a) Clear video results}
        % \label{fig:vis_clear}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/alg1/results-vis/xlarge_fontsize/result-visual-3_112.0_2021063001_li3wei3qiang2_jian4kang1cha2ti3_2021_06_30_114059_64.png}
        % \captionof{subfigure}{(b) Blurred video results}
        % \label{fig:vis_blurred}
    \end{minipage}
    % \vspace{-5pt}
    \caption{定性比较结果。\textbf{上：} 清晰案例。\textbf{下：} 运动模糊案例。我们的方法更具鲁棒性。}
    \label{fig:qual_comparisons}
    % \vspace{-5pt}
\end{figure}
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.9\linewidth]{fig/alg1/false_positive_analysis.pdf}
%     \caption{False positive profile of our framework using \cite{alwassel2018diagnosing}. Left: FP error breakdown considering top-10G predictions where G is the number of ground truths. Right: The impact of error types. Background error is the top error type.}
%     \label{fig: false-positive}
% \end{figure}



\noindent \textbf{针对不稳定骨架输入的鲁棒性。}
在没有卡尔曼滤波 \cite{kalman1960new} 的情况下，骨架分支遭受严重的性能退化（mAP@Avg. 44.6\% vs. 52.7\%），表明不稳定的关键点引入了有害噪声。值得注意的是，即使使用嘈杂的骨架，我们的 \sexyname 框架仍然取得了有竞争力的结果（62.9\% mAP@Avg.），优于卡尔曼增强的双分支（62.1\%）。这种弹性源于我们的 \crossblockshort 模块自适应地抑制冗余并对齐多模态特征的能力。通过卡尔曼滤波，\sexyname 达到了峰值性能（64.3\% mAP@Avg.），突出了稳定的骨架和跨模态交互增强了鲁棒性。因此，虽然卡尔曼滤波是辅助性的，但 \sexyname 的设计确保了无论骨架稳定性如何都能获得持续增益。}



\subsection{超参数分析}
\label{sec:hyper}
% In this section, we study the effect of different hyperparameters.
\noindent\textbf{标准差 $\sigma$ 的影响。} 我们评估了用于生成骨架热图的高斯分布的标准差 $\sigma$。
图 \ref{fig:std_dev} 显示 $\sigma=4$ 实现了最佳性能，产生了 52.7\% 的峰值平均 mAP。
较低的值 ($\sigma=1,2$) 产生过度集中的热图，缺乏空间上下文，从而降低了性能。
较大的值 ($\sigma=6,8$) 表现出类似的结果，但没有提供进一步的增益。
我们选择 $\sigma=4$，因为它在关注关键解剖结构与噪声抑制之间取得了最佳平衡，这对嘈杂 VFSS 数据中的微动作定位至关重要。}

\noindent\textbf{\crossblockshort 中层数的影响。}
我们通过改变层数来探索我们的 \crossblockfull (\crossblockshort) 模块的性能。如
% Tab.~\ref{tab: CCM layer},
图~\ref{fig:ccm_layers} 所示，
使用 3 层可以获得最佳结果，产生 \textbf{64.3\%} 的平均 mAP。
增加超过 3 层会导致平均值下降，这表明更深的架构可能会过拟合或引入不必要的复杂性。相反，使用较少的层数表现也不如 3 层配置，表明平衡的深度对于有效的跨模态特征融合至关重要。

\noindent\textbf{后期融合因子 $\omega$ 的影响。}
我们在 [0.0, 1.0] 范围内以 0.1 为增量评估融合权重因子 \(\omega\)，其中 \(\omega=0.0\) 对应于仅骨架分支输出，\(\omega=1.0\) 代表纯外观分支结果。如
% Tab.~\ref{tab: omega},
图~\ref{fig:fusion_weight} 所示，
在 \(\omega=0.6\) 时实现了最佳性能。值得注意的是，两个分支都通过我们的融合模块得到了增强，基于骨架的结果从 52.7\% 提高到 54.9\%，基于外观的结果从 61.7\% 提高到 62.4\%。

\subsection{可视化}
为了定性比较性能，我们可视化了两个具有不同复杂度的示例：一个具有清晰的动作模式，另一个表现出增加定位难度的运动模糊帧。图~\ref{fig:qual_comparisons} 展示了三种方法之间的时序检测比较：Ruan \etal~\cite{ruan2023temporal} 的原始时序微动作检测方法，基于 ActionMamba 检测器复现的框架，以及我们的 \sexyname。
在视觉上较清晰的案例中（\textit{图~\ref{fig:qual_comparisons}，上}），所有方法都成功检测到了微动作，但我们的方法展示了改进的时序边界精度。
运动模糊场景（\textit{图~\ref{fig:qual_comparisons}，下}）揭示了更明显的差异。虽然原始方法~\cite{ruan2023temporal} 未能定位大多数微动作，且其 ActionMamba 版本未能定位“口腔运送”，但我们的框架检测结果在所有动作中与真值保持了高度重叠。这种对比突出了我们的方法对视觉退化的鲁棒性，这是运动模糊频繁发生的现实世界应用中的一项关键要求。


\section{本章小结}
我们介绍了 \sexyname，这是一种用于视频透视吞咽检查 (VFSS) 中时序微动作定位 (TMAL) 的新颖框架。
通过利用编码为骨架热图的、具有临床意义的解剖学先验，我们的方法增强了对空间噪声的鲁棒性，并提高了动作定位的可解释性。
我们进一步提出了 \crossblockfull (\crossblockshort) 模块，通过线性复杂度建模和 \channelblockfull (\channelblockshort) 机制，实现了外观特征和骨架特征的高效且有效的融合。
在数据集上的广泛实验表明，SG-Mamba 取得了最先进的性能，以显著优势优于以往的方法，并展示了在跨临床环境中的优越泛化能力。这些结果凸显了骨架引导建模在推进自动化和可靠的吞咽障碍评估方面的潜力。